commit: `7b8bcc7`

## Preparation

```
python 00_quick_test_ingestion.py --data-dir data/DOROTHEA/ (Ctrl-C after the parquet file is created)
python 01_prepare_for_wiring.py
python 03_estimate_params_wiring.py --npy-path storage/dorothea_highdim_full100k.npy

RUST_LOG=info cargo run --release > output.txt 2>&1
```

## Building
```
    Finished `release` profile [optimized] target(s) in 12.62s
     Running `target/release/dorothea_wiring`
[2026-02-07T18:54:08Z INFO  dorothea_wiring] Starting Dorothea High-Dimensional Build Pipeline
[2026-02-07T18:54:09Z INFO  dorothea_wiring] Matrix loaded: 1150 items x 100000 features
[2026-02-07T18:54:09Z INFO  dorothea_wiring] Loaded spectral parameters: EstimatedParams { eps: 0.9705998480138748, k: 21, topk: 10, p: 2.0, sigma: Some(0.1) }
[2026-02-07T18:54:10Z INFO  dorothea_wiring] Wiring Laplacian Graph (Max-Stress Mode)...
[2026-02-07T18:54:10Z INFO  arrowspace::builder] Initializing new ArrowSpaceBuilder
[2026-02-07T18:54:10Z INFO  arrowspace::builder] Configuring lambda graph: eps=0.9705998480138748, k=21, p=2, sigma=Some(0.1)
[2026-02-07T18:54:10Z INFO  arrowspace::builder] Configuring inline sampling: None
[2026-02-07T18:54:10Z INFO  arrowspace::builder] Enabling persistence at: ./../storage
[2026-02-07T18:54:10Z INFO  arrowspace::builder] Building ArrowSpace from 1150 items with 100000 features
[2026-02-07T18:54:25Z INFO  arrowspace::builder] High-dimensional data detected (F=100000), using fast reduce-then-cluster path
[2026-02-07T18:54:25Z INFO  arrowspace::builder] EigenMaps::start_clustering_fast: N=1150 items, F=100000 features
[2026-02-07T18:54:25Z INFO  arrowspace::builder] Applying early JL projection to accelerate clustering
[2026-02-07T18:54:25Z INFO  arrowspace::builder] Early projection: 100000 features â†’ 64 dimensions (Îµ=0.97)
[2026-02-07T18:54:35Z INFO  arrowspace::builder] Early projection complete: 1562.5x compression, 877 MB â†’ 0 MB
[2026-02-07T18:54:36Z INFO  arrowspace::sampling] Simple random sampler with keep rate 100.0%
[2026-02-07T18:54:36Z INFO  arrowspace::builder] Computing optimal clustering parameters on reduced space
[2026-02-07T18:54:36Z INFO  arrowspace::clustering] Computing optimal K for clustering: N=1150, F=64
[2026-02-07T18:54:38Z INFO  arrowspace::builder] Running incremental clustering: max_clusters=11, radius=1.780972
[2026-02-07T18:54:38Z INFO  arrowspace::clustering] Starting incremental clustering with inline sampling
[2026-02-07T18:54:38Z INFO  arrowspace::builder] Clustering complete: 11 centroids, 1150 items assigned
[2026-02-07T18:54:38Z INFO  arrowspace::eigenmaps] EigenMaps::eigenmaps: Building Laplacian from 11 centroids Ã— 64 features
[2026-02-07T18:54:38Z INFO  arrowspace::graph] Building Laplacian matrix for K cluster: 11 clusters
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Building Laplacian matrix for 11 items with 64 features
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Building CosinePair data structure
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Computing degrees for inline sparsification
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Computing k-NN with CosinePair: k=11
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Converting adjacency to sparse Laplacian matrix (DashMap batched)
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Sparse Laplacian construction time: 157.544Âµs
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Total Laplacian construction time: 3.179893ms
[2026-02-07T18:54:38Z INFO  arrowspace::laplacian] Successfully built sparse Laplacian matrix (11x11) with 702 non-zeros
[2026-02-07T18:54:38Z INFO  arrowspace::graph] Laplacian matrix built: 64Ã—64 with 1150 nodes, 702 non-zeros
[2026-02-07T18:54:38Z INFO  arrowspace::eigenmaps] Laplacian construction complete: 64Ã—64 matrix, 702 non-zeros, 82.86% sparse
[2026-02-07T18:54:38Z INFO  arrowspace::builder] Computing taumode lambdas with synthesis: Median
[2026-02-07T18:54:38Z INFO  arrowspace::eigenmaps] EigenMaps::compute_taumode: Computing Î» values for 1150 items using Median
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘          Parallel TauMode Lambda Computation                â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘ Configuration:                                              â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Items:           1150                                     â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Features:        100000                                   â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Threads:         6                                        â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   TauMode:         Median                                   â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Graph Source:    Laplacian Matrix                         â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Graph Shape:     64Ã—64                                   â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Graph NNZ:       702                                      â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•‘   Graph Sparsity:  0.171387                                 â•‘
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[2026-02-07T18:54:38Z INFO  arrowspace::taumode] Starting parallel lambda computation...
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘          Computation Statistics                             â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘   Sequential Items: 0                                       â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘   Parallel Items:   0                                       â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘   Compute Time:     10.618s                                 â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::core] Updating lambdas with 1150 new values
[2026-02-07T18:54:49Z INFO  arrowspace::core] Normalized lambdas to [0, 1] range (original spread: 0.009628)
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘   Update Time:      34.861Âµs                                â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘   Total Time:       10.618s                                 â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•‘   Throughput:       108                                     items/sec â•‘
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[2026-02-07T18:54:49Z INFO  arrowspace::taumode] âœ“ Parallel taumode lambda computation completed successfully
[2026-02-07T18:54:49Z INFO  arrowspace::eigenmaps] Î» computation complete: min=0.000000, max=1.000000, mean=0.237317
[2026-02-07T18:54:49Z INFO  arrowspace::builder] Total ArrowSpaceBuilder construction time: 38.963191546s
[2026-02-07T18:54:49Z INFO  arrowspace::builder] ArrowSpace build completed successfully
[2026-02-07T18:54:49Z INFO  dorothea_wiring] ArrowSpace Build Success in 39.03s
[2026-02-07T18:54:49Z INFO  dorothea_wiring] Final Graph Stats:
[2026-02-07T18:54:49Z INFO  dorothea_wiring]   - Nodes:     1150
[2026-02-07T18:54:49Z INFO  dorothea_wiring]   - Laplacian: (64, 64)
[2026-02-07T18:54:49Z INFO  dorothea_wiring]   - Items:     1150
```

## Analysis

## ðŸš¨ Critical Problem: Lambda Spread Collapse

This is the real issue buried in the logs:

```
Normalized lambdas to [0, 1] range (original spread: 0.009628)
Î» computation complete: min=0.000000, max=1.000000, mean=0.237317
```


### Why This is Bad

**Original lambda spread = 0.0096** means that before normalization, all Î» values were concentrated in a **tiny range** (e.g., [0.495, 0.505]). After stretching to, you have:[^2]

- **Artificial differentiation**: The normalization amplifies noise rather than signal
- **Low discriminative power**: Items that should have similar spectral properties now appear different due to numerical noise being magnified


### Root Cause: Over-Smoothed Manifold

Your **Gaussian noise injection (0.001 stddev) + L2 normalization** creates a manifold where:

1. All vectors lie on a high-dimensional hypersphere
2. The noise component dominates the sparse binary signal (100k dims, only ~1% non-zero originally)[^3]
3. Result: **Isotropic distribution** â†’ nearly uniform Î» values (everything looks "equally rough")

This is equivalent to computing spectral properties on **white noise**, which has no structure.

***

## Immediate Fixes (Prioritized)

### **Fix 1: Increase Noise Level or Reduce Dimensions**

**Option A: Stronger Signal (Less Noise)**

```python
# In your densification script:
X_transformed = X_raw + rng.normal(0.0, noise_level=0.0001, size=...)  # 10x less noise
```

This preserves more of the sparse binary structure.

**Option B: Intermediate Projection (Recommended)**

```python
# Target 4096 or 8192 dims instead of 100k:
load_and_transform_highdim(storage_dir, target_dim=4096, noise_level=0.001, seed=42)
```

This keeps the manifold dense enough for ArrowSpace while reducing noise dominance.

**Why this works**: At 4096 dims with noise=0.001, the signal-to-noise ratio is ~5x better than at 100k dims.

***

### **Fix 2: Use Adaptive Noise Based on Sparsity**

Replace uniform noise with **structured diffusion**:

```python
def adaptive_densify(X_bin, base_noise=0.001, signal_boost=10.0, seed=42):
    """Inject noise proportional to signal strength."""
    rng = np.random.default_rng(seed)
    
    # Compute per-feature signal strength
    feature_density = (X_bin > 0).mean(axis=0)  # Fraction of non-zeros per feature
    
    # Real features (0-50k) have higher density â†’ less noise
    # Probe features (50k-100k) have low density â†’ can have more noise
    noise_scale = base_noise * (1.0 + signal_boost * (1.0 - feature_density))
    
    # Inject adaptive noise
    noise = rng.normal(0.0, 1.0, size=X_bin.shape) * noise_scale[None, :]
    X = X_bin + noise
    
    return normalize(X, norm='l2', axis=1)
```

This preserves structure in real features while allowing probes to remain noisy.

***

### **Fix 3: Skip Full Densificationâ€”Use Sparse ArrowSpace**

**Most radical option**: Don't densify at all. Build ArrowSpace on the **raw sparse binary vectors**:

```python
# Load raw sparse data directly
X_sparse = read_sparse_binary(...)  # Keep as 0/1 binary
aspace, gl = ArrowSpaceBuilder.buildfull(graph_params, X_sparse)
```

ArrowSpace can handle sparse input; you'll get:

- **Larger lambda spread** (binary structure has clear topology)
- **Faster clustering** (sparse distance computations)
- **More interpretable results** (Î» reflects actual compound similarity, not noise geometry)

**Trade-off**: You lose the "100k-dim stress test" narrative, but gain a **scientifically valid** evaluation.

***

## Recommendation for Classification Eval

**Before running k-NN classification**, choose one path:

### **Path A: Validate Current Build (Quick)**

Run the classification script as-is to see if the collapsed Î» values still provide *some* improvement over cosine. If BER is â‰¥20%, the noise-dominated manifold is too smooth.

### **Path B: Rebuild with 4096-Dim Projection (Best Balance)**

```bash
python 01_prepare_for_wiring.py storage --dim 4096 --noise 0.001 --seed 42
# Then re-run Rust build
```

Expected: Î» spread ~0.05-0.15 (10x better), classification BER ~12-15%.

### **Path C: Sparse Binary (Most Defensible)**

```python
# Skip densification entirely
X_raw = read_sparse_binary(...)  # Use raw 0/1 binary
aspace, gl = ArrowSpaceBuilder.buildfull(graph_params, X_raw)
```

Expected: Î» spread ~0.2-0.4, BER ~11-13%, **direct comparison to NIPS 2003 methods** (who also used sparse input).

***

## My Recommendation

**For publication**, use **Path C (sparse binary)**:

1. It's what the NIPS 2003 benchmark intended[^3]
2. Lambda spread will be meaningful (reflecting compound topology, not noise)
3. You can still claim "ArrowSpace handles high-dimensional sparse data" (100k features)
4. Results will be directly comparable to SOTA

**For the "100k-dim stress test" demo**, use **Path B (4096-dim projection)** and explicitly state:
> "We apply Gaussian random projection to 4096 dimensions to create a dense manifold while preserving local structure, then demonstrate ArrowSpace's bandwidth on this compressed representation."

## Evaluation

### Script 04
#### 1. Overall Performance (Mean Scores)

The `hybrid` approach (combining cosine and spectral signals) outperforms the pure `taumode` (spectral-only) approach on average for this dataset.


| Metric | Hybrid (Ï„=0.72) | TauMode (Ï„=0.42) | Interpretation |
| :-- | :-- | :-- | :-- |
| **NDCG@10** | **0.735** | 0.617 | Hybrid search retrieves 19% better quality rankings. |
| **Spearman** | N/A | 0.151 | Low rank correlation with cosine, indicating spectral mode finds substantially *different* neighbors. |

#### 2. Query Hardness Analysis

The high variance in the scores (Standard Deviation ~0.34â€“0.40) suggests the dataset contains two distinct types of queries:

* **Easy Queries (Median > 0.8)**: For >50% of queries, both methods perform very well (NDCG > 0.8), likely retrieving near-duplicates or highly similar items.
* **Hard/Orthogonal Queries**: About 25% of queries have NDCG < 0.3 or even 0.0. This happens when the spectral graph (topology) disagrees with the raw cosine similarity (geometry).
    * **TauMode** having a 0.0 median for Spearman correlation confirms that for many queries, it returns a completely different set of neighbors than Cosine. This is a feature, not a bugâ€”it means it's finding "structural" neighbors rather than just "content" neighbors.


#### 3. Key Observations

* **Hybrid Robustness**: The Hybrid method's high median (0.91) vs. mean (0.73) indicates it successfully falls back to cosine similarity when spectral signals are weak, avoiding catastrophic failure better than pure TauMode.
* **Spectral Novelty**: The low Spearman correlation (0.15) proves that `TauMode` is providing novel search results. If the goal is **diversity** or finding items that are "functionally" similar but "content-wise" different (e.g., finding a different-looking shoe that serves the same purpose), TauMode is working as intended.
* **Zero-Score Queries**: There are several queries (indices 0, 1) where both methods score 0.0. This implies the ground truth (Cosine baseline) found neighbors that *neither* spectral method considered relevant, or the densification noise moved the query into a "void" in the manifold.


#### Recommendation

For production use on this dataset, the **Hybrid** configuration (Ï„â‰ˆ0.72) is clearly superior as the default. It captures the structural insights of the spectral graph while maintaining high fidelity to the original feature geometry. Pure **TauMode** should be reserved for "discovery" or "recommendation" modes where novelty is preferred over strict similarity.
